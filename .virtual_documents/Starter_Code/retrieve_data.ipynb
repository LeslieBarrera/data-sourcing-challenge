


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json
import os
from datetime import datetime
## Load the NASA_API_KEY from the env file
load_dotenv()
NASA_API_KEY = os.getenv('NASA_API_KEY')





# Set the base URL to NASA's DONKI API:
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for CMEs:
CME = "CME"

# Search for CMEs published between a begin and end date
startDate = "2013-05-01"
endDate   = "2024-05-01"

# Build URL for CME
query_url_CME = f"{base_url}{CME}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"

# Print the query URL to confirm it is correct
print("Query URL for CME data:", query_url_CME)


# Make a "GET" request for the CME URL and store it in a variable named cme_response
cme_response = requests.get(query_url_CME)

# Print the status code to verify if the request was successful
print("Status Code:", cme_response.status_code)


# Convert the response variable to JSON and store it as a variable named cme_json
cme_json = cme_response.json()

# Print a preview of the JSON data to verify
print("Preview of cme_json:")
print(json.dumps(cme_json[:1], indent=4))  # Display the first CME event in a formatted way


# Preview the first result in JSON format
# Use json.dumps with argument indent=4 to format data
print("Formatted preview of the first CME event in JSON format:")
print(json.dumps(cme_json[0], indent=4))


# Convert cme_json to a Pandas DataFrame
cme_df = pd.DataFrame(cme_json)

# Keep only the columns: activityID, startTime, linkedEvents
cme_df = cme_df[['activityID', 'startTime', 'linkedEvents']]

# Preview the first few rows of the DataFrame to ensure it's correct
print(cme_df.head())


# Convert gst_json to a Pandas DataFrame
gst_df = pd.DataFrame(gst_json)

# Keep only the columns: gstID, startTime, linkedEvents
gst_df = gst_df[['gstID', 'startTime', 'linkedEvents']]

# Preview the first few rows of the DataFrame to ensure it's correct
print(gst_df.head())


# Remove rows with missing 'linkedEvents' since we won't be able to assign these to GSTs
cme_df_cleaned = cme_df.dropna(subset=['linkedEvents'])

# Preview the cleaned DataFrame to ensure rows with missing 'linkedEvents' are removed
print("Preview after removing rows with missing 'linkedEvents':")
print(cme_df_cleaned.head())


# Initialize an empty list to store the expanded rows
expanded_rows = []

# Iterate over each index in the DataFrame
for i in cme_df_cleaned.index:
    # Get the values from 'activityID' and 'startTime'
    activityID = cme_df_cleaned.loc[i, 'activityID']
    startTime = cme_df_cleaned.loc[i, 'startTime']
    
    # Get the list of dictionaries from 'linkedEvents'
    linkedEvents = cme_df_cleaned.loc[i, 'linkedEvents']
    
    # Check if linkedEvents is not None and is a list
    if isinstance(linkedEvents, list):
        # Iterate over each dictionary in the linkedEvents list
        for event in linkedEvents:
            # Append a new dictionary to the expanded_rows list for each event
            expanded_rows.append({
                'activityID': activityID,
                'startTime': startTime,
                'linkedEventID': event.get('activityID', None)  # Safely get activityID
            })

# Create a new DataFrame from the expanded rows
expanded_df = pd.DataFrame(expanded_rows)

# Preview the expanded DataFrame to ensure it processed correctly
print("Expanded DataFrame preview:")
print(expanded_df.head())


# Create a function called extract_activityID_from_dict
def extract_activityID_from_dict(event_dict):
    try:
        # Attempt to get the 'activityID' from the dictionary
        return event_dict.get('activityID', None)
    except Exception as e:  # Catch any exception
        # Log the error or print it for debugging
        print(f"Error extracting activityID: {e}")
        return None  # Return None in case of an error

# Test the function using one row from 'linkedEvents' as an example
# Assuming 'expanded_df' already exists and has a valid 'linkedEventID' column
example_event = {'activityID': '2013-05-04T04:52:00-IPS-001'}  # Example dictionary from linkedEvents

# Apply the function to the example
print(extract_activityID_from_dict(example_event))


# Apply this function to each row in the 'linkedEvents' column
# and create a new column called 'GST_ActivityID' using loc indexer
cme_df_cleaned.loc[:, 'GST_ActivityID'] = cme_df_cleaned['linkedEvents'].apply(
    lambda x: extract_activityID_from_dict(x[0]) if isinstance(x, list) and len(x) > 0 else None
)

# Preview the updated DataFrame to ensure the new column has been added correctly
print("Updated DataFrame with 'GST_ActivityID':")
print(cme_df_cleaned.head())


# Remove rows with missing 'GST_ActivityID' since we can't assign these to GSTs
cme_df_cleaned = cme_df_cleaned.dropna(subset=['GST_ActivityID'])

# Preview the cleaned DataFrame to ensure rows with missing 'GST_ActivityID' are removed
print("Preview after removing rows with missing 'GST_ActivityID':")
print(cme_df_cleaned.head())


# Print out the datatype of each column in the DataFrame
print(cme_df_cleaned.dtypes)


# Convert the 'GST_ActivityID' column to string format
cme_df_cleaned['GST_ActivityID'] = cme_df_cleaned['GST_ActivityID'].astype(str)

# Convert 'startTime' to datetime format (if not already done)
cme_df_cleaned['startTime'] = pd.to_datetime(cme_df_cleaned['startTime'])

# Rename 'startTime' to 'startTime_CME' and 'activityID' to 'cmeID'
cme_df_cleaned = cme_df_cleaned.rename(columns={'startTime': 'startTime_CME', 'activityID': 'cmeID'})

# Drop the 'linkedEvents' column
cme_df_cleaned = cme_df_cleaned.drop(columns=['linkedEvents'])

# Verify that all steps were executed correctly
print(cme_df_cleaned.dtypes)  # Check the datatypes
print(cme_df_cleaned.head())   # Preview the updated DataFrame


# Keep only rows where 'GST_ActivityID' contains 'GST'
cme_gst_df = cme_df_cleaned[cme_df_cleaned['GST_ActivityID'].str.contains('GST', na=False)]

# Preview the filtered DataFrame
print("Filtered CME DataFrame with GSTs:")
print(cme_gst_df.head())





# Set the base URL to NASA's DONKI API:
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for Geomagnetic Storms (GST):
GST = "GST"

# Search for GSTs between a begin and end date
startDate = "2013-05-01"
endDate   = "2024-05-01"

# Build URL for GST
query_url_GST = f"{base_url}{GST}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"


# Make a "GET" request for the GST URL and store it in a variable named gst_response
gst_response = requests.get(query_url_GST)


# Convert the response variable to json and store it as a variable named gst_json
gst_json = gst_response.json()

# Preview the first result in JSON format
# Use json.dumps with argument indent=4 to format data
import json
formatted_preview = json.dumps(gst_json[:1], indent=4)
print(formatted_preview)


# Convert gst_json to a Pandas DataFrame  
gst_df = pd.DataFrame(gst_json)

# Keep only the columns: gstID, startTime, linkedEvents
gst_df = gst_df[['gstID', 'startTime', 'linkedEvents']]

# Preview the DataFrame
print("GST DataFrame preview:")
print(gst_df.head())


# Notice that the linkedEvents column allows us to identify the corresponding CME
# Remove rows with missing 'linkedEvents' since we can't assign these to CMEs
gst_df_cleaned = gst_df.dropna(subset=['linkedEvents'])

# Preview the cleaned DataFrame to ensure rows with missing 'linkedEvents' are removed
print("Cleaned GST DataFrame preview:")
print(gst_df_cleaned.head())


# Notice that the linkedEvents sometimes contains multiple events per row
# Remove rows with missing 'linkedEvents' since we won't be able to assign these to CMEs
gst_df_cleaned = gst_df.dropna(subset=['linkedEvents'])

# Use the explode method to expand the 'linkedEvents' column
gst_df_exploded = gst_df_cleaned.explode('linkedEvents').reset_index(drop=True)

# Drop any remaining rows with missing 'linkedEvents' after the explode operation
gst_df_exploded = gst_df_exploded.dropna(subset=['linkedEvents'])

# Define the extract_activityID_from_dict function
def extract_activityID_from_dict(input_dict):
    try:
        return input_dict.get('activityID', None)
    except (ValueError, TypeError) as e:
        print(f"Error extracting activityID: {e}")
        return None

# Apply the function to each row in the 'linkedEvents' column
gst_df_exploded['CME_ActivityID'] = gst_df_exploded['linkedEvents'].apply(
    lambda x: extract_activityID_from_dict(x) if isinstance(x, dict) else None
)

# Remove rows with missing CME_ActivityID, since we can't assign them to CMEs
gst_df_exploded = gst_df_exploded.dropna(subset=['CME_ActivityID'])

# Preview the cleaned GST DataFrame
print("Cleaned and exploded GST DataFrame:")
print(gst_df_exploded.head())


# Apply the extract_activityID_from_dict function to each row in the 'linkedEvents' column
# and create a new column called 'CME_ActivityID' using loc indexer
expanded_gst_df_exploded['CME_ActivityID'] = expanded_gst_df_exploded['linkedEvents'].apply(
    lambda x: extract_activityID_from_dict(x) if pd.notna(x) else None
)

# Remove rows with missing CME_ActivityID, since we can't assign them to CMEs:
expanded_gst_df_exploded = expanded_gst_df_exploded.dropna(subset=['CME_ActivityID'])

# Preview the updated DataFrame to ensure the new column has been added and missing values removed
print("Updated exploded GST DataFrame after adding 'CME_ActivityID':")
print(expanded_gst_df_exploded.head())


# Convert the 'CME_ActivityID' column to string format 
expanded_gst_df_exploded['CME_ActivityID'] = expanded_gst_df_exploded['CME_ActivityID'].astype(str)

# Convert the 'gstID' column to string format 
expanded_gst_df_exploded['gstID'] = expanded_gst_df_exploded['gstID'].astype(str)

# Convert startTime to datetime format  
expanded_gst_df_exploded['startTime'] = pd.to_datetime(expanded_gst_df_exploded['startTime'])

# Rename startTime to startTime_GST 
expanded_gst_df_exploded.rename(columns={'startTime': 'startTime_GST'}, inplace=True)

# Drop linkedEvents
expanded_gst_df_exploded.drop(columns=['linkedEvents'], inplace=True)

# Verify that all steps were executed correctly
print("Data types after conversion:")
print(expanded_gst_df_exploded.dtypes)

# Preview the updated DataFrame
print("Updated exploded GST DataFrame:")
print(expanded_gst_df_exploded.head())


# We are only interested in GSTs related to CMEs so keep only rows where the CME_ActivityID column contains 'CME'
# Use the method 'contains()' from the str library.
gst_df_filtered = expanded_gst_df_exploded[expanded_gst_df_exploded['CME_ActivityID'].str.contains('CME', na=False)]

# Preview the filtered DataFrame
print("Filtered GST DataFrame with CMEs:")
print(gst_df_filtered)





# Now merge both datasets using 'gstID' and 'CME_ActivityID' for gst 
# and 'GST_ActivityID' and 'cmeID' for cme. Use the 'left_on' and 'right_on' specifiers.
merged_df = pd.merge(
    gst_df_filtered,                   # GST DataFrame filtered for CMEs
    cme_df_cleaned,                   # CME DataFrame cleaned earlier
    left_on='CME_ActivityID',         # Column from gst_df_filtered
    right_on='cmeID',                 # Column from cme_df_cleaned
    how='inner'                       # Use 'inner' to keep only matching rows
)

# Preview the merged DataFrame
print("Merged DataFrame preview:")
print(merged_df.head())
print(f"Number of rows in Merged DataFrame: {len(merged_df)}")


# Verify that the new DataFrame has the same number of rows as cme and gst
# Get the number of rows in the original CME and GST DataFrames
num_rows_cme = cme_df_cleaned.shape[0]
num_rows_gst = gst_df_filtered.shape[0]

# Get the number of rows in the merged DataFrame
num_rows_merged = merged_df.shape[0]

# Print the results
print(f"Number of rows in the original CME DataFrame: {num_rows_cme}")
print(f"Number of rows in the original GST DataFrame: {num_rows_gst}")
print(f"Number of rows in the merged DataFrame: {num_rows_merged}")

# Verify if the merged DataFrame has the same number of rows as CME and GST DataFrames
if num_rows_merged == (num_rows_cme + num_rows_gst):
    print("The merged DataFrame has the expected number of rows.")
else:
    print("The merged DataFrame does not have the expected number of rows.")





# Compute the time diff between startTime_GST and startTime_CME
merged_df['timeDiff'] = merged_df['startTime_GST'] - merged_df['startTime_CME']

# Preview the updated DataFrame with the new timeDiff column
print("Updated merged DataFrame with 'timeDiff':")
print(merged_df[['gstID', 'cmeID', 'timeDiff']].head())


# Compute the mean and median time difference
time_diff_in_seconds = merged_df['timeDiff'].dt.total_seconds()

# Adding mean and median to the DataFrame description
mean_time = time_diff_in_seconds.mean()
median_time = time_diff_in_seconds.median()

# Print the results
print("Mean time for a CME to cause a GST (in seconds):", mean_time)
print("Median time for a CME to cause a GST (in seconds):", median_time)

# Optionally, you can use describe() to get a full statistical summary
summary_stats = time_diff_in_seconds.describe()
print("\nStatistical summary of time differences:")
print(summary_stats)





# Export merged DataFrame to CSV
merged_df.to_csv('cme_gst_data.csv', index=False)



