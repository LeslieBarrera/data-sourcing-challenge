


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json
import os
from datetime import datetime
## Load the NASA_API_KEY from the env file
load_dotenv()
NASA_API_KEY = os.getenv('NASA_API_KEY')





# Set the base URL to NASA's DONKI API:
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for CMEs:
CME = "CME"

# Build URL for CME
query_url_CME = f"{base_url}{CME}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"



# Debugging step: Confirm the cell is running
print("Starting the API request...")

# Function to generate a list of date ranges
from datetime import timedelta, datetime

def get_date_ranges(start_date, end_date, delta=timedelta(days=30)):
    current_date = start_date
    while current_date < end_date:
        yield current_date, min(current_date + delta, end_date)
        current_date += delta

# Set the start and end dates for the data retrieval
start_date = datetime(2013, 5, 1)
end_date = datetime(2024, 5, 1)

# Empty DataFrame to store all data
all_data = pd.DataFrame()

# Loop through the smaller date ranges (30 days per request)
for start, end in get_date_ranges(start_date, end_date):
    # Format the dates as strings for the API request
    start_str = start.strftime('%Y-%m-%d')
    end_str = end.strftime('%Y-%m-%d')

    print(f"Fetching data from {start_str} to {end_str}")

    # Build the URL with the smaller date range
    query_url_CME = f"{base_url}{CME}?startDate={start_str}&endDate={end_str}&api_key={NASA_API_KEY}"

    # Make the GET request for the CME URL and set a timeout of 120 seconds
    try:
        cme_response = requests.get(query_url_CME, timeout=120)  # 2 minutes timeout

        # Print the status code to confirm if the request was successful or not
        print(f"Status Code: {cme_response.status_code}")

        if cme_response.status_code == 200:
            print(f"Data from {start_str} to {end_str} retrieved successfully!")
            cme_json = cme_response.json()

            # Convert the JSON data to a DataFrame and append it to all_data
            cme_df = pd.DataFrame(cme_json)
            all_data = pd.concat([all_data, cme_df], ignore_index=True)

        else:
            print(f"Error fetching data for {start_str} to {end_str}: {cme_response.status_code}")

    except requests.exceptions.Timeout:
        print(f"Request timed out for {start_str} to {end_str}")
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")

# Once all requests are done, display the total number of rows collected
print(f"Total rows collected: {len(all_data)}")

# Optional: Preview the first few rows of the collected data
all_data.head()

# Optional: Save the collected data to a CSV file for later use
all_data.to_csv('all_cme_data.csv', index=False)


# Print the number of rows
print(f"Total rows of data collected: {len(all_data)}")


# Preview the first few rows
all_data.head()


# Check for missing data
print(all_data.isnull().sum())


# Print the colum names
print(all_data.columns)


print(json.dumps(cme_json[:1], indent=4))  # Preview the first CME event in JSON format


all_data.head()  # Preview the first few rows of the collected data


# Preview the first result in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(cme_json[:1], indent=4))


# Convert cme_json to a Pandas DataFrame 
cme_df = pd.DataFrame(cme_json)

# Keep only the columns: activityID, startTime, linkedEvents
cme_df = cme_df[['activityID', 'startTime', 'linkedEvents']]

# Preview the first few rows of the DataFrame to ensure it's correct
cme_df.head()


# Remove rows with missing 'linkedEvents' since we can't assign these to GSTs
cme_df_cleaned = cme_df.dropna(subset=['linkedEvents'])

# Preview the cleaned DataFrame to ensure rows with missing 'linkedEvents' are removed
cme_df_cleaned.head()


# Initialize an empty list to store the expanded rows
expanded_rows = []

# Iterate over each index in the DataFrame
for i in cme_df_cleaned.index:
    # Get the corresponding value from row i in 'activityID' and 'startTime'
    activityID = cme_df_cleaned.loc[i, 'activityID']
    startTime = cme_df_cleaned.loc[i, 'startTime']
    
    # Get the list of dictionaries from row i in 'linkedEvents'
    linkedEvents = cme_df_cleaned.loc[i, 'linkedEvents']
    
    # Iterate over each dictionary in the linkedEvents list
    for event in linkedEvents:
        # Append a new dictionary to the expanded_rows list for each dictionary item
        expanded_rows.append({
            'activityID': activityID,
            'startTime': startTime,
            'linkedEventID': event.get('activityID')  # Adjust to match the structure of linkedEvents
        })

# Create a new DataFrame from the expanded rows
expanded_df = pd.DataFrame(expanded_rows)

# Preview the new expanded DataFrame
expanded_df.head()


# Create the function to extract 'activityID' from a dictionary
def extract_activityID_from_dict(event_dict):
    try:
        # Attempt to get the 'activityID' from the dictionary
        return event_dict.get('activityID', None)
    except (ValueError, TypeError) as e:
        # Log or print the error for debugging purposes
        print(f"Error extracting activityID: {e}")
        return None  # Return None in case of an error

# Test the function using one row from 'linkedEvents' as an example
# Assuming 'cme_df_cleaned' already exists and has a valid 'linkedEvents' column
example_event = cme_df_cleaned['linkedEvents'].iloc[0][0]  # Get the first dictionary in the first row of 'linkedEvents'

# Apply the function to the example
print(extract_activityID_from_dict(example_event))


# Use .loc to explicitly assign a new column 'GST_ActivityID'
cme_df_cleaned.loc[:, 'GST_ActivityID'] = cme_df_cleaned['linkedEvents'].apply(lambda x: extract_activityID_from_dict(x[0]))

# Preview the DataFrame to ensure the new column has been added correctly
cme_df_cleaned.head()


# Remove rows with missing 'GST_ActivityID'
cme_df_cleaned = cme_df_cleaned.dropna(subset=['GST_ActivityID'])

# Preview the cleaned DataFrame to ensure rows with missing 'GST_ActivityID' are removed
cme_df_cleaned.head()


# Print the datatype of each column in the DataFrame
print(cme_df_cleaned.dtypes)


# Convert 'GST_ActivityID' to string format
cme_df_cleaned['GST_ActivityID'] = cme_df_cleaned['GST_ActivityID'].astype(str)

# Convert 'startTime' to datetime format
cme_df_cleaned['startTime'] = pd.to_datetime(cme_df_cleaned['startTime'])

# Rename 'startTime' to 'startTime_CME' and 'activityID' to 'cmeID'
cme_df_cleaned = cme_df_cleaned.rename(columns={'startTime': 'startTime_CME', 'activityID': 'cmeID'})

# Drop the 'linkedEvents' column
cme_df_cleaned = cme_df_cleaned.drop(columns=['linkedEvents'])

# Verify that all steps were executed correctly by checking the first few rows and datatypes
print(cme_df_cleaned.dtypes)  # Verify column datatypes
cme_df_cleaned.head()  # Preview the updated DataFrame


# Keep only rows where 'GST_ActivityID' contains 'GST'
cme_gst_df = cme_df_cleaned[cme_df_cleaned['GST_ActivityID'].str.contains('GST', na=False)]

# Preview the filtered DataFrame to ensure the rows contain 'GST'
cme_gst_df.head()





# Set the base URL to NASA's DONKI API
base_url = "https://api.nasa.gov/DONKI/"

# Set the specifier for Geomagnetic Storms (GST)
GST = "GST"

# Search for GSTs between the start and end date
startDate = "2013-05-01"
endDate   = "2024-05-01"

# Build URL for GST
query_url_GST = f"{base_url}{GST}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"

# Preview the URL to ensure it's correct
print(query_url_GST)



# Make a "GET" request for the GST URL and store it in a variable named gst_response
gst_response = requests.get(query_url_GST)

# Check if the request was successful
if gst_response.status_code == 200:
    print("GST data retrieved successfully!")
else:
    print(f"Error: {gst_response.status_code}")


# Convert the response variable to json and store it as a variable named gst_json
gst_json = gst_response.json()

# Preview the first result in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(gst_json[:1], indent=4))


# Convert gst_json to a Pandas DataFrame
gst_df = pd.DataFrame(gst_json)

# Keep only the columns: 'gstID', 'startTime', 'linkedEvents'
gst_df = gst_df[['gstID', 'startTime', 'linkedEvents']]

# Preview the first few rows to ensure the DataFrame is correct
gst_df.head()


# Remove rows with missing 'linkedEvents' since we can't assign these to CMEs
gst_df_cleaned = gst_df.dropna(subset=['linkedEvents'])

# Preview the cleaned DataFrame to ensure rows with missing 'linkedEvents' are removed
gst_df_cleaned.head()


# Use the explode method to ensure that each row is one element in 'linkedEvents'
gst_df_exploded = gst_df_cleaned.explode('linkedEvents').reset_index(drop=True)

# Remove any remaining rows with missing 'linkedEvents' after the explode operation
gst_df_exploded = gst_df_exploded.dropna(subset=['linkedEvents'])

# Preview the DataFrame to ensure the rows have been exploded correctly
gst_df_exploded.head()


# Apply the extract_activityID_from_dict function to extract the CME 'activityID' from the 'linkedEvents' column
gst_df_exploded.loc[:, 'CME_ActivityID'] = gst_df_exploded['linkedEvents'].apply(lambda x: extract_activityID_from_dict(x))

# Remove rows with missing 'CME_ActivityID' since we can't assign these to CMEs
gst_df_exploded = gst_df_exploded.dropna(subset=['CME_ActivityID'])

# Preview the DataFrame to ensure the new column is added and rows with missing 'CME_ActivityID' are removed
gst_df_exploded.head()


# Convert 'CME_ActivityID' to string format
gst_df_exploded['CME_ActivityID'] = gst_df_exploded['CME_ActivityID'].astype(str)

# Convert 'gstID' to string format
gst_df_exploded['gstID'] = gst_df_exploded['gstID'].astype(str)

# Convert 'startTime' to datetime format
gst_df_exploded['startTime'] = pd.to_datetime(gst_df_exploded['startTime'])

# Rename 'startTime' to 'startTime_GST'
gst_df_exploded = gst_df_exploded.rename(columns={'startTime': 'startTime_GST'})

# Drop the 'linkedEvents' column as it's no longer needed
gst_df_exploded = gst_df_exploded.drop(columns=['linkedEvents'])

# Verify that all steps were executed correctly
print(gst_df_exploded.dtypes)  # Check the data types of all columns
gst_df_exploded.head()  # Preview the first few rows of the DataFrame


# Keep only rows where 'CME_ActivityID' contains 'CME'
gst_df_filtered = gst_df_exploded[gst_df_exploded['CME_ActivityID'].str.contains('CME', na=False)]

# Preview the filtered DataFrame to ensure the rows contain 'CME'
gst_df_filtered.head()





# Merge GST and CME datasets after ensuring data exists
merged_df = pd.merge(
    gst_df_filtered,
    cme_df_cleaned, 
    left_on=['gstID', 'CME_ActivityID'],
    right_on=['GST_ActivityID', 'cmeID'],
    how='inner'
)

# Check number of rows in merged DataFrame
print(f"Number of rows in Merged DataFrame: {len(merged_df)}")


# Check unique values in key columns for both datasets
print("Unique gstID and CME_ActivityID in GST data:")
print(gst_df_filtered[['gstID', 'CME_ActivityID']].drop_duplicates())

print("\nUnique GST_ActivityID and cmeID in CME data:")
print(cme_df_cleaned[['GST_ActivityID', 'cmeID']].drop_duplicates())



# Re-fetch the CME data and check if it's returning the expected data
startDate = "2023-01-01"
endDate = "2023-01-07"  # Try a smaller range to see if the request works
query_url_CME = f"{base_url}{CME}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"
cme_response = requests.get(query_url_CME, timeout=60)

# Convert response to DataFrame
cme_json = cme_response.json()
cme_df = pd.DataFrame(cme_json)

# Check the first few rows to verify the data
print(cme_df.head())



# Merge the GST and CME datasets
merged_df = pd.merge(
    gst_df_filtered,  # GST dataset
    cme_df_cleaned,   # CME dataset
    left_on=['gstID', 'CME_ActivityID'],  # Columns in GST data
    right_on=['GST_ActivityID', 'cmeID'],  # Columns in CME data
    how='inner'  # Use 'inner' to keep only matching rows, or 'outer' for all rows
)

# Preview the merged DataFrame to ensure the merge worked as expected
merged_df.head()


# Extract just the date part of the CME_ActivityID and GST_ActivityID for comparison
gst_df_filtered['CME_Date'] = gst_df_filtered['CME_ActivityID'].str[:10]
cme_df_cleaned['CME_Date'] = cme_df_cleaned['cmeID'].str[:10]

# Preview the new columns
print(gst_df_filtered[['CME_ActivityID', 'CME_Date']].head())
print(cme_df_cleaned[['cmeID', 'CME_Date']].head())



# Re-check the CME data retrieval
query_url_CME = f"{base_url}{CME}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}"
cme_response = requests.get(query_url_CME)
cme_json = cme_response.json()

# Convert to DataFrame
cme_df = pd.DataFrame(cme_json)

# Preview the first few rows of the new CME data
cme_df.head()



# Check the number of rows in each DataFrame
print(f"Number of rows in CME DataFrame: {len(cme_df_cleaned)}")
print(f"Number of rows in GST DataFrame: {len(gst_df_filtered)}")
print(f"Number of rows in Merged DataFrame: {len(merged_df)}")

# Verify if the number of rows in the merged DataFrame matches those in cme and gst DataFrames
if len(merged_df) == len(cme_df_cleaned) and len(merged_df) == len(gst_df_filtered):
    print("The merged DataFrame has the same number of rows as the original CME and GST DataFrames.")
else:
    print("The merged DataFrame does not have the same number of rows as the original CME and GST DataFrames.")





# Compute the time diff between startTime_GST and startTime_CME by creating a new column called `timeDiff`.



# Use describe() to compute the mean and median time 
# that it takes for a CME to cause a GST. 






# Export data to CSV without the index

